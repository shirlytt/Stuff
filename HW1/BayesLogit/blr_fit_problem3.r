log.target.density <- function(beta,m,y,X,beta.0,S.inv0=diag(2),S.det  = 1){    # X is of dim n*2   n <- length(y)   p <- length(beta.0)   beta = beta - beta.0   miyi  = rep(0,n)   my = cbind(m,y)   for (i in 1:n){     miyi[i] = log(factorial(my[1])/(factorial(my[2])*factorial(my[1]-my[2])))   }    log.density = - p/2*log(2*pi) - 0.5*t(beta)%*%S.inv0%*%beta  - 0.5*log(S.det) +                   sum(as.matrix(X)%*%beta*y) -  sum(m*log(1+exp(as.matrix(X)%*%beta))) + sum(miyi)                    return(log.density)}# formu = paste(names(X)[2:p],sep='', collapse = '+')      lrfit.data = cbind(X,y,n)     names(lrfit.data) = c(names(X),'y','n')     lrfit <- glm( cbind(y, n-y) ~  area+compactness+concavepts+concavity+fracdim+perimeter+radius+smoothness+symmetry+texture, data = lrfit.data, family = binomial)     beta.curr <- lrfit$coefbayes.logreg <- function(n,y,X,beta.0,Sigma.0.inv,niter=10000,burin=5000,                           print.every=1000,retune=500,verbose=TRUE){	total.samples <- niter +  burin    # Store the samples:    gibbs.samples <- matrix(NA,nrow=total.samples,ncol=length(beta.0))       # n <- length(y)    p <- length(beta.0)    S.det = 1/(det(Sigma.0.inv))    ## Specify initial values    # Starting state:     #     beta1.curr <- lrfit$coef[1]#     beta0.curr <- lrfit$coef[2]     # Track tha acceptance rate:     n.accept <- rep(0,p)     n.accept.retuned <- n.accept     # Proposal distribution: univariate normal -- N(beta0.prop-beta0.curr, v^2) [jumping step]     v <- summary(lrfit)$coefficients[,2]#     v2 <- 1.14#     rho <- 0.9999    # within each Gibbs sampler, implement MH           ######################################################           # MCMC using Metropolis/Metropolis-Hastings Algorithm           ######################################################     do.metropolis <- TRUE    do.MH <- FALSE        if (verbose){       cat("\n=============================================\n")       cat("Implementing Metropolis Algorithm...\n")       cat("=============================================\n\n")      }     # Metropolis-Markov Chains Algorithm within Gibbs samplers:   if (do.metropolis){total.samples    for (i in 1:total.samples){        beta.prop <- beta.curr        for (ip in 1:p){          # Propose a new state in the ip-th parameter :          beta.prop.ip  <- beta.prop          beta.prop.ip[ip] <- rnorm(n=1,mean=beta.curr[ip],sd=v)          # Compute the log-acceptance probability:           log.density.prop = log.target.density(beta=beta.prop.ip,m=m,y=y,X=X,beta.0=beta.0,S.inv0=Sigma.0.inv,S.det  = S.det)          log.density.curr = log.target.density(beta=beta.prop,m=m,y=y,X=X,beta.0=beta.0,S.inv0=Sigma.0.inv,S.det = S.det)          log.alpha <-  log.density.prop  -   log.density.curr          # Decide whether to accept or reject:          log.u <- log(runif(1))          if (log.u < log.alpha){              # Accept:              beta.prop[ip] <- beta.prop.ip[ip]              n.accept[ip] <- n.accept[ip] + 1           } else {               # Reject            }         }    # ip (parameter index) -loop           beta.curr <- beta.prop         # Store the current state:         gibbs.samples[i,] <- beta.curr                           # Check acceptance rate and whether to tune v       if (i%% retune == 0  & i<burin){            for ( j in 1:length(n.accept)){                # if (n.accept[j]/i<0.3){                if ((n.accept[j]-n.accept.retuned[j])/retune<0.3){                    vold = v[j]                    v[j] = v[j]/2                    if (verbose)                    cat(paste('Iteration ',i,' ,par ', j,': current acceptance rate is ', round(100*n.accept[j]/i)/100, ', change v from ',vold,' to ',v[j],'\n',sep=''))                 }                   # if (n.accept[j]/i>0.7){                if ((n.accept[j]-n.accept.retuned[j])/retune>0.7){                    vold = v[j]                    v[j] = v[j] * 2                    if (verbose)                    cat(paste('Iteration ',i,' ,par ', j,': current acceptance rate is ', round(100*n.accept[j]/i)/100, ', change v from ',vold, ' to ',v[j],'\n',sep=''))                 }                  }            n.accept.retuned = n.accept          }       # print an update to the user      if (i%%print.every == 0 & verbose){      cat(paste("This is iteration",i,"and the new sampled beta is",          gibbs.samples[i,1],gibbs.samples[i,2],"\n",sep=" "))      }     }   return(list(n.accept=n.accept, sample = gibbs.samples[(burin+1):total.samples,]))   }      # Metropolis-Markov Chains Algorithm by itself   if (do.MH){            Sigma.prop = v* matrix(c(1,rho*sqrt(v2),rho*sqrt(v2),v2),nrow=2)*0.01      beta.curr = c(beta0.curr,beta1.curr)      for (i in 1:total.samples){        # Propose a new state:        beta.prop <- rmvnorm(n=1,mean=beta.curr,sigma = Sigma.prop)        beta.prop = t(beta.prop)        # Compute the log-acceptance probability: log(alpha) = log( pi(theta^prop)/pi(theta^curr) )        log.alpha <- log.target.density(beta.prop,m=m,y=y,X=X,beta.0=beta.0,S.inv0=Sigma.0.inv,S.det  = S.det) -                   log.target.density(beta.curr,m=m,y=y,X=X,beta.0=beta.0,S.inv0=Sigma.0.inv,S.det = S.det)        # Decide whether to accept or reject:        log.u <- log(runif(1))        if (log.u < log.alpha){          # Accept:          beta.curr <- beta.prop          n.accept[1] <- n.accept[1] + 1         } else {          # Reject         }        # Store the current state:        gibbs.samples[i,] <- beta.curr        # Check acceptance rate and whether to tune v         if (i%% retune == 0  & i<burin){           if(any(n.accept[1]/i<0.3, n.accept[2]/i<0.3)){              v = v / 2            }            if (any(n.accept[1]/i>0.6, n.accept[2]/i>0.6)){              v = v * 2              }          if (verbose & any(n.accept[1]/i<0.3, n.accept[2]/i<0.3,n.accept[1]/i>0.6, n.accept[2]/i>0.6)){           cat(paste('This is iteration ',i,', current acceptance rate is ', round(100*n.accept[1]/i)/100, ', change v to ',v,'\n',sep=''))          }         }        # print an update to the user      if (i%%print.every == 0 & verbose ){      cat(paste("This is iteration",i,"and the new sampled beta is [",          gibbs.samples[i,1],gibbs.samples[i,2],"]\n",sep=" "))      }     }   return(list(n.accept=n.accept, sample = gibbs.samples[(burin+1):total.samples,]))   }}library(mvtnorm) # For rmvnormlibrary(MASS)# Read data data = read.table('breast_cancer.txt',header=TRUE)var  =  names(data) p = 10data$y = as.numeric(data$diagnosis == "M")data$n = rep(1,length(data$y)) X    = sapply(data, function(x){(x-mean(x))/sd(x)})[,1:p]  dat = data.frame(cbind(X,as.numeric(data$diagnosis == "M"),rep(1,length(data$y)))) #  #  names(dat) = c(var[1:10], 'y','n') ################################################## Set up the specifications:p     <-  10beta.0 <- matrix(rep(0,p+1))Sigma.0.inv <- ginv(diag(rep(1000.0,p+1)))niter <- 10000burin <- 5000print.every  <- 1000retune      <-  500verbose <- TRUE# etc... (more needed here)################################################## Extract X and y:m = data$ny = data$yX = as.matrix(cbind(rep(1,dim(dat)[1]),dat[,1:10]))# Fit the Bayesian model:  # set the initial value  lrfit <- glm( cbind(y, n-y) ~  area+compactness+concavepts+concavity+fracdim+perimeter+radius+smoothness+symmetry+texture, data = dat, family = binomial)beta.curr <- lrfit$coefresult = bayes.logreg(m,y,X,beta.0,Sigma.0.inv,niter,burin,print.every,retune,verbose)## predictve checking:gibbs = gibbs.samples[5001:15000,]smp = sample(10000, 100)pred.mean = rep(NA, 100)pred.median = rep(NA, 100)pred.max = rep(NA, 100)for (i in 1:100){   beta.pred = gibbs[smp[i],]   #################################################      p <- length(beta.pred)   #################################################   eta <- X%*%beta.pred   y <- rbinom(n=5000,size=1,prob=exp(eta)/(1+exp(eta)))   pred.mean[i] = mean(dat$y)}